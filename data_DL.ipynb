{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_conf = ps.SparkConf()\\\n",
    "            .set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\\\n",
    "            .set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "            .set(\"spark.sql.shuffle.partitions\",200)\n",
    "            # '_started'と'_committed_'で始まるファイルを書き込まないように設定\n",
    "            # '_SUCCESS'で始まるファイルを書き込まないように設定\n",
    "            # パーティション数を増やす\n",
    "spark = SparkSession.builder.config(conf=ps_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(join(os.getcwd(), '.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_PYSPARK = '/mnt/adintedataexplorer'\n",
    "BASE_PATH_PANDAS  = '/dbfs/mnt/adintedataexplorer'\n",
    "SILVER_PATH       = '_ml-medallion/dev/test_silver/'\n",
    "MITSUBISHI_CONF   = 'csv_data/mitsubishi_japan_motor_show.csv'\n",
    "NAGAOKA_HNB_CONF  = 'csv_data/nagaoka_hanabi.csv'\n",
    "AKA_RENGA_CONF    = 'csv_data/yokohama_aka_renga.csv'\n",
    "MITSUBISHI_CSV    = 'csv_data/beacon_for_mitsubishi.csv'\n",
    "NAGAOKA_HNB_CSV   = 'csv_data/beacon_for_nagaoka.csv'\n",
    "AKA_RENGA_CSV     = 'csv_data/beacon_for_aka_renga.csv'\n",
    "SPECIFIED_COMB    = (MITSUBISHI_CONF, MITSUBISHI_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = BASE_PATH_PYSPARK + SILVER_PATH + SPECIFIED_COMB[0]\n",
    "df_csv = spark.read\\\n",
    "                .option('inferSchema', 'True')\\\n",
    "                .option('header', 'True')\\\n",
    "                .csv(path)\n",
    "\n",
    "df_csv.display()\n",
    "utid_list = sorted(df_csv.select(\"unit_id\").drop_duplicates().rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DAY = '2023-10-20'\n",
    "END_DAY   = '2023-11-16'\n",
    "AIBEACON_PATH = 'adinte.aibeacon_wifi_log'\n",
    "\n",
    "df_raw_data = spark.table(AIBEACON_PATH)\\\n",
    "                    .withColumn('date',     F.to_date(col('date')))\\\n",
    "                    .withColumn('datetime', F.to_timestamp(col('datetime')))\\\n",
    "                    .filter((col('date') >= START_DAY) & (col('date') <= END_DAY))\\\n",
    "                    .filter(col('randomized') == '1')\\\n",
    "                    .filter(col('unit_id').isin(utid_list))\n",
    "df_raw_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = BASE_PATH_PANDAS + SILVER_PATH + SPECIFIED_COMB[1]\n",
    "pd_raw_data = df_raw_data.toPandas()\\\n",
    "                .to_csv(path, index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
