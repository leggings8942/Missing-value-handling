{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.contrib import tenumerate\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_conf = ps.SparkConf()\\\n",
    "            .set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\\\n",
    "            .set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "            .set(\"spark.sql.shuffle.partitions\",200)\n",
    "            # '_started'と'_committed_'で始まるファイルを書き込まないように設定\n",
    "            # '_SUCCESS'で始まるファイルを書き込まないように設定\n",
    "            # パーティション数を増やす\n",
    "spark = SparkSession.builder.config(conf=ps_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_by1min(start_date:str, end_date:str):\n",
    "    # 開始日時と終了日時を設定\n",
    "    start_time = f'{start_date} 00:00:00'\n",
    "    end_time   = f'{end_date} 23:59:00'\n",
    "    # 1分単位の時間列を作成\n",
    "    time_range = pd.date_range(start=start_time, end=end_time, freq='min')\n",
    "    \n",
    "    # DataFrameに変換\n",
    "    pd_time = pd.DataFrame(time_range, columns=['datetime'])\n",
    "    # display(df_time)\n",
    "    \n",
    "    spark = SparkSession.getActiveSession()\n",
    "    return spark.createDataFrame(pd_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MITSUBISHI    = 'csv_data/beacon_for_mitsubishi.csv'\n",
    "AKA_RENGA     = 'csv_data/beacon_aka_renga.csv'\n",
    "df_csv = spark.read\\\n",
    "                .option('inferSchema', 'True')\\\n",
    "                .option('header', 'True')\\\n",
    "                .csv(MITSUBISHI)\\\n",
    "                .withColumn(\"datetime\", F.date_trunc(\"minute\", col(\"datetime\")))\\\n",
    "                .groupBy(\"unit_id\", \"date\", \"datetime\").count()\\\n",
    "                .select([\"unit_id\", \"date\", \"datetime\", \"count\"])\n",
    "utid_list = sorted(df_csv.select(\"unit_id\").drop_duplicates().rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "pd_datetime = df_csv.select(F.max(\"date\").alias(\"max\"), F.min(\"date\").alias(\"min\")).toPandas()\n",
    "start_date, end_date = pd_datetime[\"min\"].tolist()[0].strftime('%Y-%m-%d'), pd_datetime[\"max\"].tolist()[0].strftime('%Y-%m-%d')\n",
    "\n",
    "df_1min = create_time_series_by1min(start_date, end_date)\n",
    "for unit_id in utid_list:\n",
    "    df_tmp  = df_csv.filter(col(\"unit_id\") == unit_id)\\\n",
    "        \t\t.withColumnRenamed(\"count\", unit_id)\\\n",
    "\t\t\t\t.select([\"datetime\", unit_id])\n",
    "    df_1min = df_1min.join(df_tmp, on=\"datetime\", how=\"left\")\n",
    "df_1min = df_1min.orderBy(\"datetime\")\n",
    "\n",
    "print(\"start_date:\", start_date)\n",
    "print(\"end_date:\", end_date)\n",
    "df_1min.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2023-11-2 3:00\"\n",
    "end_date   = \"2023-11-3 3:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_counter = df_1min.filter((col(\"datetime\") >= start_date) & (col(\"datetime\") <= end_date)).toPandas()\n",
    "date_list  = pd_counter[\"datetime\"].tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 36))\n",
    "for idx, unit_id in enumerate(utid_list):\n",
    "    axes = fig.add_subplot(len(utid_list), 1, idx + 1)\n",
    "    \n",
    "    count_list = pd_counter[unit_id].tolist()\n",
    "    axes.plot(date_list, count_list, \"-\")\n",
    "    axes.set_title(unit_id)\n",
    "    axes.set_xlabel(\"時刻\")\n",
    "    axes.set_ylabel(\"ビーコン計測数\")\n",
    "    axes.grid(True)\n",
    "\n",
    "print(\"unit_id list:\", utid_list)\n",
    "print(\"nanの数：\", np.sum(np.isnan(count_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import changefinder\n",
    "import ruptures as rpt\n",
    "\n",
    "pd_counter = df_1min.filter((col(\"datetime\") >= start_date) & (col(\"datetime\") <= end_date)).toPandas()\n",
    "date_list  = pd_counter[\"datetime\"].tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 36))\n",
    "for idx, unit_id in enumerate(utid_list):\n",
    "    axes = fig.add_subplot(len(utid_list), 1, idx + 1)\n",
    "    \n",
    "    w1 = 5\n",
    "    \n",
    "    count_list = pd_counter[unit_id].tolist()\n",
    "    count_list = np.cumsum(np.nan_to_num(count_list))\n",
    "    axes.plot(date_list, count_list, \"-\", label=\"AI Beacon反応数の積算値\", color=\"blue\")\n",
    "    \n",
    "    axes2  = axes.twinx()\n",
    "    cf     = changefinder.ChangeFinder(r=0.01, order=1, smooth=7)\n",
    "    scores = [cf.update(point) for point in count_list]\n",
    "    axes2.plot(date_list, scores, \"-\", label=\"ChangeFinder Score\", color=\"green\")\n",
    "    \n",
    "    algo = rpt.Pelt(model='l2').fit(count_list)\n",
    "    my_bkps = algo.predict(pen=0.1)\n",
    "    algo_x = [date_list[idx]  for idx in my_bkps[:-1]]\n",
    "    algo_y = [count_list[idx] for idx in my_bkps[:-1]]\n",
    "    axes.plot(algo_x, algo_y, \"o\", label=\"Pruned Exact Linear Time検出点\", color=\"red\")\n",
    "    \n",
    "    axes.set_title(unit_id)\n",
    "    axes.set_xlabel(\"時刻\")\n",
    "    axes.set_ylabel(\"ビーコン計測数\")\n",
    "    axes.grid(True)\n",
    "    axes.legend()\n",
    "    axes2.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "print(\"unit_id list:\", utid_list)\n",
    "print(\"nanの数：\", np.sum(np.isnan(count_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(w1:int, x_list, y_list):\n",
    "    if len(x_list) != len(y_list):\n",
    "        raise ValueError()\n",
    "    \n",
    "    mov_ave = np.convolve(y_list, [1 / w1] * w1, mode='same')\n",
    "    return x_list[w1:-w1], mov_ave[w1:-w1]\n",
    "\n",
    "def diff_convolve(w1:int, x_list, y_list):\n",
    "    if len(x_list) != len(y_list):\n",
    "        raise ValueError()\n",
    "    \n",
    "    conv_list = np.convolve(y_list, [0.5, 0, -0.5], mode='full')\n",
    "    tuple_xy  = moving_average(w1, x_list[1:-1], conv_list[2:-2])\n",
    "    return tuple_xy\n",
    "\n",
    "def arg_extremum(w1:int, α1:int, x_list, y_list):\n",
    "    if len(x_list) != len(y_list):\n",
    "        raise ValueError()\n",
    "    \n",
    "    x_list, y_list = diff_convolve(w1, x_list, y_list)\n",
    "    x_list, y_list = diff_convolve(w1, x_list, y_list)\n",
    "    \n",
    "    convol_y  = np.diff(y_list)\n",
    "    sign_list = np.sign(convol_y[:-1] * convol_y[1:])\n",
    "    base_std  = np.std(y_list)\n",
    "    \n",
    "    sign_idx  = np.where(((sign_list == -1) & (convol_y[1:] > 0)) & (y_list[1:-1] < -α1 * base_std))[0]\n",
    "    extre_min = 1 + sign_idx\n",
    "    \n",
    "    sign_idx  = np.where(((sign_list == -1) & (convol_y[1:] < 0)) & (y_list[1:-1] >  α1 * base_std))[0]\n",
    "    extre_max = 1 + sign_idx\n",
    "    \n",
    "    return extre_min + 2 * (w1 + 1), extre_max + 2 * (w1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_counter = df_1min.filter((col(\"datetime\") >= start_date) & (col(\"datetime\") <= end_date)).toPandas()\n",
    "date_list  = pd_counter[\"datetime\"].tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 36))\n",
    "for idx, unit_id in enumerate(utid_list):\n",
    "    axes  = fig.add_subplot(len(utid_list), 1, idx + 1)\n",
    "    axes2 = axes.twinx()\n",
    "    \n",
    "    w1 = 11\n",
    "    α1 = 2.17\n",
    "    \n",
    "    # 標準正規分布の確立分布　数表\n",
    "    # URL:https://kyozaikenkyu-statistics.blog.jp/%E6%A8%99%E6%BA%96%E6%AD%A3%E8%A6%8F%E5%88%86%E5%B8%83%E6%95%B0%E8%A1%A8.pdf\n",
    "    # 主に以下の値が利用されると想定する\n",
    "    # 優位水準8%   (片側4.0%) ・・・1.75\n",
    "    # 優位水準5%   (片側2.5%) ・・・1.96\n",
    "    # 優位水準3%   (片側1.5%) ・・・2.17\n",
    "    # 優位水準1%   (片側0.5%) ・・・2.58\n",
    "    # 優位水準0.5% (片側0.25%)・・・2.81\n",
    "    \n",
    "    count_list = pd_counter[unit_id].tolist()\n",
    "    cumsum_list = np.cumsum(np.nan_to_num(count_list))\n",
    "    d_list, c_list = moving_average(w1, date_list, cumsum_list)\n",
    "    axes2.plot(d_list, c_list, \"-\", label=\"AI Beacon反応数の積算値\", color=\"blue\")\n",
    "    \n",
    "    axes.plot(date_list, count_list, \"-\", label=\"元データ\",       color=\"gray\")\n",
    "    d_list, c_list = diff_convolve(w1, date_list, cumsum_list)\n",
    "    axes.plot(d_list,    c_list,     \"-\", label=\"積算値の１階微分\", color=\"brown\")\n",
    "    d_list, c_list = diff_convolve(w1, d_list, c_list)\n",
    "    axes.plot(d_list,    c_list,     \"-\", label=\"積算値の２階微分\", color=\"orange\")\n",
    "    \n",
    "    extre_min, extre_max = arg_extremum(w1, α1, date_list, cumsum_list)\n",
    "    extre_x = [date_list[idx]   for idx in extre_min]\n",
    "    extre_y = [cumsum_list[idx] for idx in extre_min]\n",
    "    axes2.plot(extre_x, extre_y, \"o\", label=\"変化点の始値候補\", color=\"lime\")\n",
    "    extre_x = [date_list[idx]   for idx in extre_max]\n",
    "    extre_y = [cumsum_list[idx] for idx in extre_max]\n",
    "    axes2.plot(extre_x, extre_y, \"o\", label=\"変化点の終値候補\", color=\"red\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    axes.set_title(unit_id)\n",
    "    axes.set_xlabel(\"時刻\")\n",
    "    axes.set_ylabel(\"ビーコン計測数\")\n",
    "    axes.grid(True)\n",
    "    axes.legend()\n",
    "    axes2.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "print(\"unit_id list:\", utid_list)\n",
    "print(\"nanの数：\", np.sum(np.isnan(count_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GLMM_model import LogitLinearRegression\n",
    "\n",
    "pd_counter = df_1min.filter((col(\"datetime\") >= start_date) & (col(\"datetime\") <= end_date)).toPandas()\n",
    "date_list  = pd_counter[\"datetime\"].tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 36))\n",
    "for idx, unit_id in tenumerate(utid_list):\n",
    "    axes  = fig.add_subplot(len(utid_list), 1, idx + 1)\n",
    "    axes2 = axes.twinx()\n",
    "    \n",
    "    count_list = pd_counter[unit_id].tolist()\n",
    "    axes.plot(date_list, count_list, \"-\", label=\"元データ\", color=\"gray\")\n",
    "    \n",
    "    cumsum_list = np.cumsum(np.nan_to_num(count_list))\n",
    "    d_list, c_list = moving_average(w1, date_list, cumsum_list)\n",
    "    axes2.plot(d_list, c_list, \"-\", label=\"AI Beacon反応数の積算値\", color=\"blue\")\n",
    "    \n",
    "    extre_min, extre_max = arg_extremum(w1, α1, date_list, cumsum_list)\n",
    "    extre_x = [date_list[idx]   for idx in extre_min]\n",
    "    extre_y = [cumsum_list[idx] for idx in extre_min]\n",
    "    axes2.plot(extre_x, extre_y, \"o\", label=\"変化点の始値候補\", color=\"lime\")\n",
    "    extre_x = [date_list[idx]   for idx in extre_max]\n",
    "    extre_y = [cumsum_list[idx] for idx in extre_max]\n",
    "    axes2.plot(extre_x, extre_y, \"o\", label=\"変化点の終値候補\", color=\"red\")\n",
    "    \n",
    "    # 標準正規分布の確立分布　数表\n",
    "    # URL:https://kyozaikenkyu-statistics.blog.jp/%E6%A8%99%E6%BA%96%E6%AD%A3%E8%A6%8F%E5%88%86%E5%B8%83%E6%95%B0%E8%A1%A8.pdf\n",
    "    # 主に以下の値が利用されると想定する\n",
    "    # 優位水準8%   (片側4.0%) ・・・1.75\n",
    "    # 優位水準5%   (片側2.5%) ・・・1.96\n",
    "    # 優位水準3%   (片側1.5%) ・・・2.17\n",
    "    # 優位水準1%   (片側0.5%) ・・・2.58\n",
    "    # 優位水準0.5% (片側0.25%)・・・2.81\n",
    "    \n",
    "    x_list = list(map(lambda x: x.timestamp() - date_list[0].timestamp(), date_list))\n",
    "    x_list = np.array(x_list).reshape([len(x_list), 1])\n",
    "    model  = LogitLinearRegression(isStandardization=True, window_size=11, norm_z=2.17, random_state=100)\n",
    "    model.fit(x_list, cumsum_list, visible_flg=True)\n",
    "    \n",
    "    pred_list = model.predict(x_list)\n",
    "    axes2.plot(date_list, pred_list, \"-\", label=\"AI Beacon反応数の補正値\", color=\"brown\")\n",
    "    \n",
    "    d_list, c_list = diff_convolve(11, date_list, pred_list)\n",
    "    axes.plot(d_list, c_list, \"-\", label=\"補正値の１階微分\", color=\"orange\")\n",
    "    \n",
    "    \n",
    "    axes.set_title(unit_id)\n",
    "    axes.set_xlabel(\"時刻\")\n",
    "    axes.set_ylabel(\"ビーコン計測数\")\n",
    "    axes.grid(True)\n",
    "    axes.legend()\n",
    "    axes2.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "print(\"unit_id list:\", utid_list)\n",
    "print(\"nanの数：\", np.sum(np.isnan(count_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
